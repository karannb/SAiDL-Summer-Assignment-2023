{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "84941a4f",
   "metadata": {},
   "source": [
    "# SAiDL 2023, Core ML Bonus Part"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ed02381",
   "metadata": {},
   "source": [
    "## Transformer based architecture in place of a CNN"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6efb6f8d",
   "metadata": {},
   "source": [
    "### Packages -"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "8914f193",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import torch.optim as optim\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import time\n",
    "\n",
    "torch.random.manual_seed(1)\n",
    "np.random.seed(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b197d8f8",
   "metadata": {},
   "source": [
    "### HyperParameters -"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5ff52a43",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 1\n",
    "num_epochs = 10\n",
    "mha_heads = 4"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbcaf8eb",
   "metadata": {},
   "source": [
    "### General Transform -"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b96ccdce",
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = transforms.Compose(\n",
    "    [transforms.ToTensor(),\n",
    "     transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee28a6c0",
   "metadata": {},
   "source": [
    "### Dataset and DataLoader -"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c5366cfd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "tensor([[[[ 0.5608,  0.5373,  0.5294,  ...,  0.6941,  0.7020,  0.7569],\n",
      "          [ 0.5451,  0.5294,  0.5294,  ...,  0.8118,  0.8275,  0.8902],\n",
      "          [ 0.7412,  0.6706,  0.6392,  ...,  0.9059,  0.9137,  0.9608],\n",
      "          ...,\n",
      "          [-0.4353, -0.4431, -0.4196,  ...,  0.7255,  0.4353,  0.2157],\n",
      "          [-0.4353, -0.4039, -0.3412,  ...,  0.7412,  0.4667,  0.1373],\n",
      "          [-0.3725, -0.3412, -0.3333,  ...,  0.7020,  0.6235,  0.3804]],\n",
      "\n",
      "         [[ 0.6863,  0.6549,  0.6471,  ...,  0.8118,  0.8118,  0.8353],\n",
      "          [ 0.6471,  0.6314,  0.6471,  ...,  0.9059,  0.9059,  0.9216],\n",
      "          [ 0.7725,  0.7255,  0.7176,  ...,  0.9608,  0.9529,  0.9608],\n",
      "          ...,\n",
      "          [-0.4275, -0.4196, -0.3882,  ...,  0.6314,  0.3176,  0.1059],\n",
      "          [-0.4118, -0.3647, -0.3020,  ...,  0.6627,  0.3647,  0.0353],\n",
      "          [-0.3333, -0.2941, -0.2784,  ...,  0.6235,  0.5216,  0.2863]],\n",
      "\n",
      "         [[ 0.9529,  0.9137,  0.9059,  ...,  0.9608,  0.9608,  0.9765],\n",
      "          [ 0.8745,  0.8667,  0.8824,  ...,  0.9608,  0.9608,  0.9843],\n",
      "          [ 0.9294,  0.8980,  0.9059,  ...,  0.9686,  0.9686,  0.9843],\n",
      "          ...,\n",
      "          [-0.2235, -0.2000, -0.1529,  ...,  0.7020,  0.4196,  0.1765],\n",
      "          [-0.1843, -0.1294, -0.0431,  ...,  0.7255,  0.5059,  0.1686],\n",
      "          [-0.0745, -0.0353, -0.0039,  ...,  0.6863,  0.6549,  0.4353]]]]) tensor([49])\n",
      "torch.Size([1, 3, 32, 32]) torch.Size([1])\n"
     ]
    }
   ],
   "source": [
    "train_dataset = torchvision.datasets.CIFAR100(root='./data', train=True,\n",
    "                                               download=True, transform=transform)\n",
    "\n",
    "test_dataset = torchvision.datasets.CIFAR100(root='./data', train=False,\n",
    "                                            download=True, transform=transform)\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=batch_size,\n",
    "                                           shuffle=True)\n",
    "\n",
    "test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=batch_size,\n",
    "                                          shuffle=False)\n",
    "\n",
    "output_iter = iter(test_loader)\n",
    "features, labels = next(output_iter)\n",
    "print(features, labels)\n",
    "print(features.size(), labels.size())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5f0eba3",
   "metadata": {},
   "source": [
    "### Example image -"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ac678e9a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def imshow(img):\n",
    "    img = img / 2 + 0.5  # unnormalize\n",
    "    npimg = img.numpy()\n",
    "    plt.imshow(np.transpose(npimg, (1, 2, 0)))\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "# get some random training images\n",
    "dataiter = iter(train_loader)\n",
    "images, labels = next(dataiter)\n",
    "\n",
    "# show images\n",
    "imshow(torchvision.utils.make_grid(images))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba7acf07",
   "metadata": {},
   "source": [
    "### Encoder -"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "c56de596",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PatchEmbed(nn.Module):\n",
    "    def __init__(self, img_size=32, patch_size=8, in_channels=3, embed_dim=512):\n",
    "        super().__init__()\n",
    "        self.img_size = img_size\n",
    "        self.patch_size = patch_size\n",
    "        self.n_patches = (img_size//patch_size)**2\n",
    "        #print(self.n_patches)\n",
    "        \n",
    "        self.proj = nn.Conv2d(in_channels, \n",
    "                              embed_dim, \n",
    "                              kernel_size=patch_size, \n",
    "                              stride=patch_size)\n",
    "        self.cls_token = nn.Parameter(torch.randn(1, 1, embed_dim))\n",
    "        \n",
    "        self.pos_embedding = nn.Parameter(torch.randn(self.n_patches+1, embed_dim))\n",
    "        \n",
    "    def forward(self, x):\n",
    "        \n",
    "        x = self.proj(x) #(batch_size, embed_dim, n_patches**0.5, n_patches**0.5)\n",
    "        x = x.flatten(2) #(batch_size, embed_dim, n_patches)\n",
    "        x = x.transpose(1, 2) #(batch_size, n_patches, embed_dim)\n",
    "        x = torch.cat([self.cls_token, x], dim=1)\n",
    "        x += self.pos_embedding\n",
    "        \n",
    "        return x\n",
    "\n",
    "\n",
    "class MLPBlock(nn.Sequential):\n",
    "    def __init__(self, emb_size, expansion=4, drop_p= 0.1):\n",
    "        super().__init__(\n",
    "            nn.Linear(emb_size, expansion * emb_size),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(drop_p),\n",
    "            nn.Linear(expansion * emb_size, emb_size),\n",
    "        )\n",
    "\n",
    "        \n",
    "class EncoderBlock(nn.Module):\n",
    "    def __init__(self, emb_size=512, MLP_drop_p=0.1, drop_p=0.1, num_heads=4, forward_expansion=4):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.layer1 = nn.LayerNorm(emb_size)\n",
    "        self.mha = nn.MultiheadAttention(embed_dim=emb_size, num_heads=num_heads, dropout=drop_p, batch_first=True)\n",
    "        self.layer2 = nn.LayerNorm(emb_size)\n",
    "        self.mlp = MLPBlock(emb_size, expansion=forward_expansion, drop_p=MLP_drop_p)\n",
    "        self.dropout = nn.Dropout(drop_p)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        \n",
    "        res = x\n",
    "        x, _ = self.mha(self.layer1(x), self.layer1(x), self.layer1(x))\n",
    "        x += res\n",
    "        res = x\n",
    "        x = self.layer2(x)\n",
    "        x = self.mlp(x)\n",
    "        x = self.dropout(x)\n",
    "        x += res\n",
    "        \n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b4f79e6",
   "metadata": {},
   "source": [
    "### Overall Model -"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "244504ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "class VisionTransformer(nn.Module):\n",
    "    def __init__(self, depth, n_classes=100):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.depth = depth\n",
    "        self.embedder = PatchEmbed()\n",
    "        self.encoders = nn.ModuleList([])\n",
    "        for _ in range(depth):\n",
    "            self.encoders.append(EncoderBlock())\n",
    "        self.out = nn.Sequential(nn.LayerNorm(512), nn.Linear(512, n_classes))\n",
    "    \n",
    "    def forward(self, x):\n",
    "        \n",
    "        x = self.embedder(x)\n",
    "        for module in self.encoders:\n",
    "            x = module(x)\n",
    "        x = torch.sum(x, axis=1)\n",
    "        x = self.out(x)\n",
    "        \n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "5adce087",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "VisionTransformer(\n",
      "  (embedder): PatchEmbed(\n",
      "    (proj): Conv2d(3, 512, kernel_size=(8, 8), stride=(8, 8))\n",
      "  )\n",
      "  (encoders): ModuleList(\n",
      "    (0-3): 4 x EncoderBlock(\n",
      "      (layer1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "      (mha): MultiheadAttention(\n",
      "        (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)\n",
      "      )\n",
      "      (layer2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "      (mlp): MLPBlock(\n",
      "        (0): Linear(in_features=512, out_features=2048, bias=True)\n",
      "        (1): ReLU()\n",
      "        (2): Dropout(p=0.1, inplace=False)\n",
      "        (3): Linear(in_features=2048, out_features=512, bias=True)\n",
      "      )\n",
      "      (dropout): Dropout(p=0.1, inplace=False)\n",
      "    )\n",
      "  )\n",
      "  (out): Sequential(\n",
      "    (0): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "    (1): Linear(in_features=512, out_features=100, bias=True)\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "print(VisionTransformer(4))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94149bcf",
   "metadata": {},
   "source": [
    "### Loss -"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "8f767c0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def gumbel_sample(shape, eps = 1e-8):\n",
    "    \n",
    "    U = torch.rand(shape)\n",
    "    #U = U.to(device)\n",
    "    return -torch.log(-torch.log(U + eps) + eps)\n",
    "\n",
    "class loss_gumbel(nn.Module):\n",
    "    \n",
    "    def __init__(self):\n",
    "        super(loss_gumbel, self).__init__()\n",
    "    \n",
    "    def forward(self, output, target, temp):\n",
    "        \n",
    "        criterion = nn.CrossEntropyLoss()\n",
    "        y_hat = output + gumbel_sample(output.size())\n",
    "        loss = criterion(y_hat/temp, target)\n",
    "        \n",
    "        return loss"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1018e200",
   "metadata": {},
   "source": [
    "### Training -"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "b58331b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "ViT = VisionTransformer(4)\n",
    "criterion = loss_gumbel()\n",
    "optimizer = optim.SGD(ViT.parameters(), lr=1e-3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "83bfe646",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/10], Step [1/50000], Loss: 9.3257\n",
      "Epoch [1/10], Step [2/50000], Loss: 7.6250\n",
      "Epoch [1/10], Step [3/50000], Loss: 5.5436\n",
      "Epoch [1/10], Step [4/50000], Loss: 5.0114\n",
      "Epoch [1/10], Step [5/50000], Loss: 6.4892\n",
      "Epoch [1/10], Step [6/50000], Loss: 8.4927\n",
      "Epoch [1/10], Step [7/50000], Loss: 4.0464\n",
      "Epoch [1/10], Step [8/50000], Loss: 6.4015\n",
      "Epoch [1/10], Step [9/50000], Loss: 3.6842\n",
      "Epoch [1/10], Step [10/50000], Loss: 6.1821\n",
      "Epoch [1/10], Step [11/50000], Loss: 4.5147\n",
      "Epoch [1/10], Step [12/50000], Loss: 6.7881\n",
      "Epoch [1/10], Step [13/50000], Loss: 5.7600\n",
      "Epoch [1/10], Step [14/50000], Loss: 7.7245\n",
      "Epoch [1/10], Step [15/50000], Loss: 6.8620\n",
      "Epoch [1/10], Step [16/50000], Loss: 6.5871\n",
      "Epoch [1/10], Step [17/50000], Loss: 7.2910\n",
      "Epoch [1/10], Step [18/50000], Loss: 7.1655\n",
      "Epoch [1/10], Step [19/50000], Loss: 3.7437\n",
      "Epoch [1/10], Step [20/50000], Loss: 3.7043\n",
      "Epoch [1/10], Step [21/50000], Loss: 6.5489\n",
      "Epoch [1/10], Step [22/50000], Loss: 7.1812\n",
      "Epoch [1/10], Step [23/50000], Loss: 4.4534\n",
      "Epoch [1/10], Step [24/50000], Loss: 5.8015\n",
      "Epoch [1/10], Step [25/50000], Loss: 6.4974\n",
      "Epoch [1/10], Step [26/50000], Loss: 4.5405\n",
      "Epoch [1/10], Step [27/50000], Loss: 5.6838\n",
      "Epoch [1/10], Step [28/50000], Loss: 2.2177\n",
      "Epoch [1/10], Step [29/50000], Loss: 5.9674\n",
      "Epoch [1/10], Step [30/50000], Loss: 9.3007\n",
      "Epoch [1/10], Step [31/50000], Loss: 8.1334\n",
      "Epoch [1/10], Step [32/50000], Loss: 6.4058\n",
      "Epoch [1/10], Step [33/50000], Loss: 7.7178\n",
      "Epoch [1/10], Step [34/50000], Loss: 5.2848\n",
      "Epoch [1/10], Step [35/50000], Loss: 4.3326\n",
      "Epoch [1/10], Step [36/50000], Loss: 6.0488\n",
      "Epoch [1/10], Step [37/50000], Loss: 6.5219\n",
      "Epoch [1/10], Step [38/50000], Loss: 8.2407\n",
      "Epoch [1/10], Step [39/50000], Loss: 9.2106\n",
      "Epoch [1/10], Step [40/50000], Loss: 5.1127\n",
      "Epoch [1/10], Step [41/50000], Loss: 5.6163\n",
      "Epoch [1/10], Step [42/50000], Loss: 5.9418\n",
      "Epoch [1/10], Step [43/50000], Loss: 8.2994\n",
      "Epoch [1/10], Step [44/50000], Loss: 8.2744\n",
      "Epoch [1/10], Step [45/50000], Loss: 7.2057\n",
      "Epoch [1/10], Step [46/50000], Loss: 6.0633\n",
      "Epoch [1/10], Step [47/50000], Loss: 6.1660\n",
      "Epoch [1/10], Step [48/50000], Loss: 6.5378\n",
      "Epoch [1/10], Step [49/50000], Loss: 7.3575\n",
      "Epoch [1/10], Step [50/50000], Loss: 4.1158\n",
      "Epoch [1/10], Step [51/50000], Loss: 4.2561\n",
      "Epoch [1/10], Step [52/50000], Loss: 7.5297\n",
      "Epoch [1/10], Step [53/50000], Loss: 6.0816\n",
      "Epoch [1/10], Step [54/50000], Loss: 7.1424\n",
      "Epoch [1/10], Step [55/50000], Loss: 4.2204\n",
      "Epoch [1/10], Step [56/50000], Loss: 5.6393\n",
      "Epoch [1/10], Step [57/50000], Loss: 5.6729\n",
      "Epoch [1/10], Step [58/50000], Loss: 7.2440\n",
      "Epoch [1/10], Step [59/50000], Loss: 1.0538\n",
      "Epoch [1/10], Step [60/50000], Loss: 6.7081\n",
      "Epoch [1/10], Step [61/50000], Loss: 2.7213\n",
      "Epoch [1/10], Step [62/50000], Loss: 4.7422\n",
      "Epoch [1/10], Step [63/50000], Loss: 5.7207\n",
      "Epoch [1/10], Step [64/50000], Loss: 6.9596\n",
      "Epoch [1/10], Step [65/50000], Loss: 6.9398\n",
      "Epoch [1/10], Step [66/50000], Loss: 3.6191\n",
      "Epoch [1/10], Step [67/50000], Loss: 5.7067\n",
      "Epoch [1/10], Step [68/50000], Loss: 5.8892\n",
      "Epoch [1/10], Step [69/50000], Loss: 5.3261\n",
      "Epoch [1/10], Step [70/50000], Loss: 3.9983\n",
      "Epoch [1/10], Step [71/50000], Loss: 4.6459\n",
      "Epoch [1/10], Step [72/50000], Loss: 9.4624\n",
      "Epoch [1/10], Step [73/50000], Loss: 4.1892\n",
      "Epoch [1/10], Step [74/50000], Loss: 6.4036\n",
      "Epoch [1/10], Step [75/50000], Loss: 3.7105\n",
      "Epoch [1/10], Step [76/50000], Loss: 10.4733\n",
      "Epoch [1/10], Step [77/50000], Loss: 8.1564\n",
      "Epoch [1/10], Step [78/50000], Loss: 4.6945\n",
      "Epoch [1/10], Step [79/50000], Loss: 6.2284\n",
      "Epoch [1/10], Step [80/50000], Loss: 8.0183\n",
      "Epoch [1/10], Step [81/50000], Loss: 9.3631\n",
      "Epoch [1/10], Step [82/50000], Loss: 6.7068\n",
      "Epoch [1/10], Step [83/50000], Loss: 5.1968\n",
      "Epoch [1/10], Step [84/50000], Loss: 7.0640\n",
      "Epoch [1/10], Step [85/50000], Loss: 8.0066\n",
      "Epoch [1/10], Step [86/50000], Loss: 3.1214\n",
      "Epoch [1/10], Step [87/50000], Loss: 4.8071\n",
      "Epoch [1/10], Step [88/50000], Loss: 3.0201\n",
      "Epoch [1/10], Step [89/50000], Loss: 5.8359\n",
      "Epoch [1/10], Step [90/50000], Loss: 8.4584\n",
      "Epoch [1/10], Step [91/50000], Loss: 5.6038\n",
      "Epoch [1/10], Step [92/50000], Loss: 7.6420\n",
      "Epoch [1/10], Step [93/50000], Loss: 6.1666\n",
      "Epoch [1/10], Step [94/50000], Loss: 5.5315\n",
      "Epoch [1/10], Step [95/50000], Loss: 4.3824\n",
      "Epoch [1/10], Step [96/50000], Loss: 5.3950\n",
      "Epoch [1/10], Step [97/50000], Loss: 5.7358\n",
      "Epoch [1/10], Step [98/50000], Loss: 7.0026\n",
      "Epoch [1/10], Step [99/50000], Loss: 5.0455\n",
      "Epoch [1/10], Step [100/50000], Loss: 4.5627\n",
      "Epoch [1/10], Step [500/50000], Loss: 7.0719\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[76], line 16\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[38;5;66;03m# Backward and optimize\u001b[39;00m\n\u001b[1;32m     15\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[0;32m---> 16\u001b[0m \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     17\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mstep()\n\u001b[1;32m     19\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (i \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m) \u001b[38;5;241m%\u001b[39m (n_total_steps\u001b[38;5;241m/\u001b[39m\u001b[38;5;241m5\u001b[39m) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n",
      "File \u001b[0;32m~/anaconda3/envs/CLIPSeg/lib/python3.10/site-packages/torch/_tensor.py:487\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    477\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    478\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    479\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[1;32m    480\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    485\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[1;32m    486\u001b[0m     )\n\u001b[0;32m--> 487\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    488\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[1;32m    489\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/CLIPSeg/lib/python3.10/site-packages/torch/autograd/__init__.py:200\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    195\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[1;32m    197\u001b[0m \u001b[38;5;66;03m# The reason we repeat same the comment below is that\u001b[39;00m\n\u001b[1;32m    198\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    199\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 200\u001b[0m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    201\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    202\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "loss_hist = []\n",
    "iter_no = []\n",
    "n_total_steps = len(train_loader)\n",
    "times = []\n",
    "current = time.time()\n",
    "for epoch in range(num_epochs):\n",
    "    for i, (images, labels) in enumerate(train_loader):\n",
    "        # origin shape: [1, 3, 32, 32] = 1, 3, 1024\n",
    "\n",
    "        # Forward pass\n",
    "        outputs = ViT(images)\n",
    "        loss = criterion(outputs, labels, 0.9)\n",
    "\n",
    "        # Backward and optimize\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        if (i + 1) % (n_total_steps/5) == 0:\n",
    "            loss_hist.append(loss.item())\n",
    "            iter_no.append(i + epoch*n_total_steps)\n",
    "        \n",
    "        if i < 10:\n",
    "            print(f'Epoch [{epoch+1}/{num_epochs}], Step [{i+1}/{n_total_steps}], Loss: {loss.item():.4f}')\n",
    "\n",
    "        if (i + 1) % (n_total_steps/10) == 0:\n",
    "            print(f'Epoch [{epoch+1}/{num_epochs}], Step [{i+1}/{n_total_steps}], Loss: {loss.item():.4f}')\n",
    "    \n",
    "    times.append(time.time() - current)\n",
    "    current = time.time()\n",
    "        \n",
    "avg_time = sum(times)/len(times)\n",
    "\n",
    "print('Finished Training')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fe99887",
   "metadata": {},
   "source": [
    "### Model Evaluation -"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d73f731d",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(iter_no, loss_hist)\n",
    "plt.title(\"iterations vs loss\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "224b2c12",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"With gumbel softmax, each epoch lasted {avg_time}s\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6586cb21",
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    confusion_matrix = np.zeros((100, 100))\n",
    "    n_correct = 0\n",
    "    n_samples = 0\n",
    "    for images, labels in test_loader:\n",
    "        outputs = ViT(images)\n",
    "        # max returns (value ,index)\n",
    "        _, predicted = torch.max(outputs, 1)\n",
    "        for i in range(batch_size):\n",
    "            confusion_matrix[labels[i]][predicted[i]] += 1\n",
    "        n_samples += labels.size(0)\n",
    "        n_correct += (predicted == labels).sum().item()\n",
    "\n",
    "    acc = 100.0 * n_correct / n_samples\n",
    "    print(f'Accuracy of the network: {acc} %')\n",
    "    \n",
    "    plt.imshow(confusion_matrix, cmap = 'gray')\n",
    "    plt.title('CONFUSION MATRIX - ')\n",
    "    plt.show()\n",
    "    \n",
    "    print(confusion_matrix)\n",
    "    \n",
    "    precision = np.zeros((100,))\n",
    "    recall = np.zeros((100,))\n",
    "    \n",
    "    for classes in range(100):\n",
    "        if(np.sum(confusion_matrix[:,classes]) > 0):\n",
    "            precision[classes] = confusion_matrix[classes][classes]/np.sum(confusion_matrix[:,classes])\n",
    "        else:\n",
    "            precision[classes] = 0\n",
    "        if(np.sum(confusion_matrix[classes,:]) > 0):    \n",
    "            recall[classes] = confusion_matrix[classes][classes]/np.sum(confusion_matrix[classes,:])\n",
    "        else:\n",
    "            recall[classes] = 0\n",
    "    \n",
    "    max_p = precision[np.argmax(precision)]\n",
    "    \n",
    "    print(f'max precision at class - {np.argmax(precision)} = {max_p*100}%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "f18b93c4",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'confusion_matrix' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[77], line 4\u001b[0m\n\u001b[1;32m      2\u001b[0m recall \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m classes \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m100\u001b[39m):\n\u001b[0;32m----> 4\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m(np\u001b[38;5;241m.\u001b[39msum(\u001b[43mconfusion_matrix\u001b[49m[:,classes]) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m):\n\u001b[1;32m      5\u001b[0m         precision_N \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m confusion_matrix[classes][classes]\u001b[38;5;241m/\u001b[39mnp\u001b[38;5;241m.\u001b[39msum(confusion_matrix[:,classes])\n\u001b[1;32m      6\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m(np\u001b[38;5;241m.\u001b[39msum(confusion_matrix[classes,:]) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m):    \n",
      "\u001b[0;31mNameError\u001b[0m: name 'confusion_matrix' is not defined"
     ]
    }
   ],
   "source": [
    "precision = 0\n",
    "recall = 0\n",
    "for classes in range(100):\n",
    "    if(np.sum(confusion_matrix[:,classes]) > 0):\n",
    "        precision_N += confusion_matrix[classes][classes]/np.sum(confusion_matrix[:,classes])\n",
    "    if(np.sum(confusion_matrix[classes,:]) > 0):    \n",
    "        recall_N += confusion_matrix[classes][classes]/np.sum(confusion_matrix[classes,:])\n",
    "        \n",
    "precision /= 100\n",
    "recall /= 100\n",
    "\n",
    "F1 = 2/((1/precision) + (1/recall))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc45b7bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Precision - {precision:.4f}, Recall - {recall:.4f} and F1-score is - {F1:.4f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
